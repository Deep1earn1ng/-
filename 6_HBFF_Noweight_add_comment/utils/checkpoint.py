import os
import torch
import logging
import glob
import re


def save_checkpoint(cfg, epoch, model, optimizer, scheduler, is_best_map=False, is_best_rank1=False):
    """
    å®Œå–„åçš„æ¨¡å‹ä¿å­˜å‡½æ•°ï¼š
    1. çŠ¶æ€å­—å…¸åŒ…å«éšæœºç§å­ä»¥æ”¯æŒå®éªŒå¤ç°ã€‚
    2. æ”¯æŒå¤šæŒ‡æ ‡ï¼ˆmAP, Rank-1ï¼‰æœ€ä¼˜æƒé‡åˆ†åˆ«ä¿å­˜ã€‚
    3. è‡ªåŠ¨æ¸…ç†ç­–ç•¥ï¼šä»…ä¿ç•™æœ€æ–°çš„ä¸€ä¸ªå‘¨æœŸæ€§æƒé‡ï¼Œä»¥åŠä¸¤ä¸ªæœ€ä¼˜æƒé‡ã€‚
    """
    logger = logging.getLogger("reid.checkpoint")

    # å»ºç«‹ä¿å­˜ç›®å½•
    output_dir = cfg.OUTPUT_DIR
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # å‡†å¤‡ä¿å­˜çš„çŠ¶æ€å­—å…¸
    state = {
        'epoch': epoch,
        'state_dict': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'scheduler': scheduler.state_dict(),
        'cfg': cfg,
        # [ä¼˜åŒ–] ä½¿ç”¨ getattr å¢å¼ºé²æ£’æ€§ï¼Œé˜²æ­¢é…ç½®ä¸­ç¼ºå°‘ SEED å¯¼è‡´å´©æºƒ
        'seed': getattr(cfg.SOLVER, 'SEED', 1234)
    }

    # 1. ä¿å­˜å½“å‰å‘¨æœŸæƒé‡ (ä¾‹å¦‚ transformer_epoch_10.pth)
    # è¿™æ˜¯ç”¨äºæ–­ç‚¹ç»­è®­çš„åŸºç¡€æ–‡ä»¶
    filename = os.path.join(output_dir, f"{cfg.MODEL.NAME}_epoch_{epoch}.pth")
    torch.save(state, filename)

    # 2. å¦‚æœæ˜¯ mAP æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–å­˜ä¸€ä»½ (è¦†ç›–æ—§çš„ best_mAP.pth)
    if is_best_map:
        best_map_path = os.path.join(output_dir, f"{cfg.MODEL.NAME}_best_mAP.pth")
        torch.save(state, best_map_path)
        logger.info(f"ğŸ† ä¿å­˜å½“å‰ mAP æœ€é«˜æ¨¡å‹è‡³: {best_map_path}")

    # 3. å¦‚æœæ˜¯ Rank-1 æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–å­˜ä¸€ä»½ (è¦†ç›–æ—§çš„ best_rank1.pth)
    if is_best_rank1:
        best_r1_path = os.path.join(output_dir, f"{cfg.MODEL.NAME}_best_rank1.pth")
        torch.save(state, best_r1_path)
        logger.info(f"ğŸš€ ä¿å­˜å½“å‰ Rank-1 æœ€é«˜æ¨¡å‹è‡³: {best_r1_path}")

    # 4. æƒé‡æ¸…ç†ç­–ç•¥ï¼šä»…ä¿ç•™æœ€æ–°çš„ 1 ä¸ªå‘¨æœŸæ€§æ¨¡å‹
    # åŒ¹é…æ¨¡å¼: transformer_epoch_*.pth
    # æ³¨æ„: best_mAP å’Œ best_rank1 ä¸åŒ…å« "_epoch_"ï¼Œå› æ­¤ä¸ä¼šè¢«è¯¯åˆ 
    pattern = os.path.join(output_dir, f"{cfg.MODEL.NAME}_epoch_*.pth")
    checkpoint_files = glob.glob(pattern)

    # è§£ææ–‡ä»¶åä¸­çš„ epoch æ•°å­—å¹¶è¿›è¡Œæ’åº
    file_list = []
    for f in checkpoint_files:
        match = re.search(r'epoch_(\d+)\.pth', f)
        if match:
            file_list.append((int(match.group(1)), f))

    # æŒ‰ç…§ epoch ç¼–å·é™åºæ’åˆ—ï¼ˆæœ€æ–°çš„æ’åœ¨å‰é¢ï¼‰
    file_list.sort(key=lambda x: x[0], reverse=True)

    # å¦‚æœæ–‡ä»¶æ€»æ•°è¶…è¿‡ 1 ä¸ªï¼Œåˆ™åˆ é™¤æ‰€æœ‰æ›´æ—©çš„å‘¨æœŸæ€§æƒé‡
    if len(file_list) > 1:
        for _, old_file_path in file_list[1:]:
            try:
                os.remove(old_file_path)
                # ä¸å†è¾“å‡ºæ¸…ç†æ—¥å¿—ï¼Œä¿æŒæ§åˆ¶å°æ•´æ´
            except OSError as e:
                logger.warning(f"âš ï¸ åˆ é™¤æ–‡ä»¶ {old_file_path} å¤±è´¥: {e}")


def load_checkpoint(path, model, optimizer=None, scheduler=None):
    """
    åŠ è½½æ¨¡å‹æƒé‡ã€‚æ”¯æŒæ¨ç†åŠ è½½æˆ–æ–­ç‚¹ç»­è®­åŠ è½½ã€‚
    """
    logger = logging.getLogger("reid.checkpoint")
    if not os.path.exists(path):
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶: {path}")

    logger.info(f"=> æ­£åœ¨åŠ è½½æƒé‡: {path}")
    checkpoint = torch.load(path, map_location='cpu')

    # å…¼å®¹æ€§å¤„ç†ï¼šåˆ¤æ–­æ˜¯å­˜çš„å®Œæ•´ state è¿˜æ˜¯ä»… state_dict
    if 'state_dict' in checkpoint:
        model_state = checkpoint['state_dict']
    else:
        model_state = checkpoint

    # åŠ è½½åˆ°æ¨¡å‹ (strict=False å…è®¸éƒ¨åˆ†æƒé‡ä¸åŒ¹é…ï¼Œä¾‹å¦‚å¾®è°ƒæ—¶)
    msg = model.load_state_dict(model_state, strict=False)
    logger.info(f"æ¨¡å‹åŠ è½½ç»“æœ: {msg}")

    # å¦‚æœæä¾›äº†ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ï¼Œä¸” checkpoint ä¸­åŒ…å«å®ƒä»¬ï¼Œåˆ™æ¢å¤çŠ¶æ€ (ç”¨äºæ–­ç‚¹ç»­è®­)
    if optimizer is not None and 'optimizer' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer'])
        logger.info("=> æˆåŠŸæ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€")

    if scheduler is not None and 'scheduler' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler'])
        logger.info("=> æˆåŠŸæ¢å¤è°ƒåº¦å™¨çŠ¶æ€")

    return checkpoint.get('epoch', 0)


def load_pretrain_vit(model, pretrain_path):
    """
    ä¸“é—¨ç”¨äºåŠ è½½ ImageNet é¢„è®­ç»ƒçš„ ViT æƒé‡ (é€šå¸¸ä»…åŒ…å« model state_dict)ã€‚
    """
    logger = logging.getLogger("reid.checkpoint")
    if not os.path.exists(pretrain_path):
        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡: {pretrain_path}ï¼Œå°†ä»éšæœºåˆå§‹åŒ–å¼€å§‹ã€‚")
        return

    checkpoint = torch.load(pretrain_path, map_location='cpu')
    logger.info(f"=> åŠ è½½é¢„è®­ç»ƒ ViT: {pretrain_path}")

    # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœé¢„è®­ç»ƒæ–‡ä»¶ä¹Ÿæ˜¯å®Œæ•´ checkpoint
    if 'state_dict' in checkpoint:
        checkpoint = checkpoint['state_dict']

    model_dict = model.state_dict()

    # ä»…åŠ è½½å½¢çŠ¶åŒ¹é…çš„é”® (è¿‡æ»¤æ‰åˆ†ç±»å¤´ç­‰ä¸åŒ¹é…çš„å±‚)
    pretrained_dict = {k: v for k, v in checkpoint.items() if k in model_dict and v.shape == model_dict[k].shape}

    model_dict.update(pretrained_dict)
    model.load_state_dict(model_dict)

    missing_keys = set(model.state_dict().keys()) - set(pretrained_dict.keys())
    # logger.info(f"é¢„è®­ç»ƒåŒ¹é…æˆåŠŸã€‚æœªåŒ¹é…åˆ°çš„é”®: {list(missing_keys)[:5]}...")